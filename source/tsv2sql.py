import os
import re
import sqlite3
import traceback
from pathlib import Path

import pandas as pd

from common.constants import exclude_files
from source.change_coordinates import bd09togcj02
from common.config import HAN_PATH, APPEND_PATH, QUERY_DB_PATH, DIALECTS_DB_PATH, CHARACTERS_DB_PATH, PHO_TABLE_PATH, \
    MISSING_DATA_LOG, WRITE_INFO_LOG, YINDIAN_DATA_DIR
from source.get_new import extract_all_from_files
from source.match_fromdb import get_tsvs


def build_dialect_database():
    # han_file = Path(HAN_CSV_PATH)
    han_file = Path(HAN_PATH)
    other_file = Path(APPEND_PATH)
    sqlite_db = Path(QUERY_DB_PATH)

    # --- æ¬„ä½å°æ‡‰ ---
    tone_map = {
        "[1]é™°å¹³": "T1é™°å¹³",
        "[2]é™½å¹³": "T2é™½å¹³",
        "[3]é™°ä¸Š": "T3é™°ä¸Š",
        "[4]é™½ä¸Š": "T4é™½ä¸Š",
        "[5]é™°å»": "T5é™°å»",
        "[6]é™½å»": "T6é™½å»",
        "[7]é™°å…¥": "T7é™°å…¥",
        "[8]é™½å…¥": "T8é™½å…¥",
        "[9]å…¶ä»–èª¿": "T9å…¶ä»–èª¿",
        "[10]è¼•è²": "T10è¼•è²"
    }

    geo_map = {
        "çœ/è‡ªæ²»å€/ç›´è½„å¸‚": "çœ",
        "åœ°å€/å¸‚/å·": "å¸‚",
        "ç¸£/å¸‚/å€": "ç¸£",
        "é„•/é­/è¡—é“": "é®",
        "æ‘/ç¤¾å€/å±…æ°‘é»": "è¡Œæ”¿æ‘",
        "è‡ªç„¶æ‘": "è‡ªç„¶æ‘"
    }

    rename_map = {**tone_map, **geo_map}

    # æ¬„ä½æ¸…å–®ï¼ˆåŸå§‹åç¨±ï¼‰
    required_columns = [
        "èªè¨€", "ç°¡ç¨±", "éŸ³å…¸æ’åº", "åœ°åœ–é›†äºŒåˆ†å€", "éŸ³å…¸åˆ†å€", "å­—è¡¨ä¾†æºï¼ˆæ¯æœ¬ï¼‰", "æ–¹è¨€å³¶",
        "å­˜å„²æ¨™è¨˜", "ç¶“ç·¯åº¦", "åœ°åœ–ç´šåˆ¥",
        *geo_map.keys(),
        *tone_map.keys()
    ]

    # --- è®€å– Append_files.xlsx.xlsx ---
    df_other = pd.read_excel(other_file, sheet_name="æª”æ¡ˆ", header=0)
    df_other.columns = df_other.columns.str.strip()
    df_other["å­˜å„²æ¨™è¨˜"] = ""  # âœ… è£œä¸Šé€™ä¸€åˆ—
    df_other = df_other[[col for col in required_columns if col in df_other.columns]].copy()
    df_other = df_other.rename(columns=rename_map)

    # --- è®€å– æ¼¢å­—éŸ³å…¸è¡¨ï¼Œè·³éç¬¬ 2 è¡Œï¼ˆå³ index 0ï¼‰---
    df_han = pd.read_excel(han_file, sheet_name="æª”æ¡ˆ", header=0, engine='openpyxl', keep_default_na=False)
    # df_han = pd.read_csv(han_file)
    df_han = df_han.drop(index=0).reset_index(drop=True)
    df_han.columns = df_han.columns.str.strip()
    df_han["å­˜å„²æ¨™è¨˜"] = ""  # âœ… è£œä¸Šé€™ä¸€åˆ—
    df_han = df_han[[col for col in required_columns if col in df_han.columns]].copy()
    df_han = df_han.rename(columns=rename_map)

    # --- è™•ç†ç¶“ç·¯åº¦è½‰æ› ---
    def convert_coordinates(df):
        """
        å° 'ç¶“ç·¯åº¦' åˆ—é€²è¡Œåæ¨™è½‰æ›ï¼Œå¿½ç•¥ç©ºå€¼
        """
        new_coordinates = []
        for coords in df['ç¶“ç·¯åº¦']:
            # å¦‚æœç¶“ç·¯åº¦ç‚ºç©ºï¼Œè·³é
            if pd.isna(coords) or coords.strip() == '':
                new_coordinates.append(None)  # å¦‚æœæ˜¯ç©ºå€¼ï¼Œå°‡ç¶“ç·¯åº¦è¨­ç‚º None
                continue

            # ç¢ºä¿ coords æ˜¯å­—ç¬¦ä¸²é¡å‹
            coords = str(coords).strip()

            # åˆ†å‰²ç¶“ç·¯åº¦
            bd_lon, bd_lat = map(float, re.split(r'[ï¼Œ,]', coords))

            # ä½¿ç”¨è½‰æ›å‡½æ•¸
            converted_coords = bd09togcj02(bd_lon, bd_lat)
            new_coordinates.append(f"{converted_coords[0]},{converted_coords[1]}")  # è½‰æ›å¾Œçš„åæ¨™ä»¥é€—è™Ÿåˆ†éš”

        # æ›´æ–° 'ç¶“ç·¯åº¦' åˆ—
        df['ç¶“ç·¯åº¦'] = new_coordinates
        return df

    # è™•ç† df_other å’Œ df_han å…©å€‹ DataFrame
    df_other = convert_coordinates(df_other)
    df_han = convert_coordinates(df_han)

    # --- å¯«å…¥ SQLite ---
    with sqlite3.connect(sqlite_db) as conn:
        # è¨˜éŒ„ä¾†æº
        df_other["_ä¾†æº"] = "Append_files.xlsx"
        df_han["_ä¾†æº"] = "æ¼¢å­—éŸ³å…¸è¡¨"

        # åˆä½µè³‡æ–™
        merged = pd.concat([df_other, df_han], ignore_index=True)

        # âœ… åœ¨æ­¤è™•æ’å…¥æ›¿æ›é‚è¼¯
        def replace_dialect_zone(val):
            if isinstance(val, str):
                if val.startswith("å®¢å®¶è©±-ç²µåŒ—ç‰‡"):
                    return val.replace("å®¢å®¶è©±-ç²µåŒ—ç‰‡", "å®¢å®¶è©±-ç²µåŒ—ç‰‡Â·å®¢", 1)
                elif val.startswith("å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡"):
                    return val.replace("å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡", "å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡Â·åœŸ", 1)
            return val

        merged["åœ°åœ–é›†äºŒåˆ†å€"] = merged["åœ°åœ–é›†äºŒåˆ†å€"].apply(replace_dialect_zone)

        # å¾ŒçºŒè™•ç† ...

        # è½‰æ› required_columns â†’ é‡å‘½åå¾Œçš„æ¬„ä½å
        renamed_required_columns = [rename_map.get(col, col) for col in required_columns]

        # è¨ˆç®—éç©ºæ¬„ä½æ•¸
        merged["_non_null_count"] = merged[renamed_required_columns].notna().sum(axis=1)

        # å„ªå…ˆä¾†æºæ¨™è¨˜ï¼ˆæ¼¢å­—éŸ³å…¸è¡¨å„ªå…ˆï¼‰
        merged["_ä¾†æºå„ªå…ˆ"] = merged["_ä¾†æº"].apply(lambda x: 1 if x == "æ¼¢å­—éŸ³å…¸è¡¨" else 0)

        # æœ€çµ‚ä¿ç•™è³‡æ–™åˆ—è¡¨
        final_rows = []

        def get_nonnull_info(row):
            if row.empty:
                return 0, []
            # count = int(row["_non_null_count"])
            cols = [col for col in renamed_required_columns if pd.notna(row[col]) and row[col] != ""]
            count = len(cols)
            return count, cols
        print("\nğŸ“Š é‡è¤‡ç°¡ç¨±é¸æ“‡è©³æƒ…å¦‚ä¸‹ï¼š")
        for name, group in merged.groupby("ç°¡ç¨±"):
            if len(group) > 1:
                # è®¡ç®—æ¯è¡Œçš„éç©ºåˆ—æ•°å¹¶æ·»åŠ ä¸ºæ–°åˆ—
                group["count"] = group.apply(
                    lambda row: len([col for col in renamed_required_columns if pd.notna(row[col]) and row[col] != ""]),
                    axis=1)

                # é€‰æ‹© count æœ€å¤§çš„è¡Œï¼Œå¦‚æœ count ç›¸åŒåˆ™ä¼˜å…ˆé€‰æ‹© "æ¼¢å­—éŸ³å…¸è¡¨"
                selected = None
                for _, row in group.iterrows():
                    count, cols = get_nonnull_info(row)
                    if selected is None or count > selected["count"] or (
                            count == selected["count"] and row["_ä¾†æº"] == "æ¼¢å­—éŸ³å…¸è¡¨"):
                        selected = row
                        selected["count"] = count  # æ›´æ–° selected çš„ count

                final_rows.append(selected)

                print(f"\nğŸŸ¡ ç°¡ç¨±: {name}")
                for _, row in group.iterrows():
                    count, cols = get_nonnull_info(row)
                    print(f"  â¤ ä¾†æºï¼š{row['_ä¾†æº']}ï¼Œéç©ºæ¬„ä½ {count} å€‹ï¼š{', '.join(cols)}")

                print(f"  âœ… æœ€çµ‚é¸ä¸­ä¾†æºï¼š{selected['_ä¾†æº']}")
            else:
                final_rows.append(group.iloc[0])

        # å»ºç«‹æœ€çµ‚ DataFrame
        final_df = pd.DataFrame(final_rows).drop(columns=["_non_null_count", "_ä¾†æºå„ªå…ˆ", "_ä¾†æº"])
        final_df = final_df.sort_values(by="éŸ³å…¸æ’åº", na_position="last")

        # å¯«å…¥è³‡æ–™åº«
        final_df.to_sql("dialects", conn, if_exists="replace", index=False)
        # åŠ ç´¢å¼•
        conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_dialects_code ON dialects(ç°¡ç¨±);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_zone ON dialects(éŸ³å…¸åˆ†å€);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_zone ON dialects(åœ°åœ–é›†äºŒåˆ†å€);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_flag ON dialects(å­˜å„²æ¨™è¨˜);")

    print(f"âœ… SQLite è³‡æ–™åº« `dialects_query.db` å·²å»ºç«‹ï¼Œdialects è¡¨å·²æ›´æ–°å®Œæˆã€‚")


def process_all2sql(tsv_paths, db_path, append=False):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    if not append:
        cursor.execute("DROP TABLE IF EXISTS dialects")
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS dialects (
            ç°¡ç¨± TEXT,
            æ¼¢å­— TEXT,
            éŸ³ç¯€ TEXT,
            è²æ¯ TEXT,
            éŸ»æ¯ TEXT,
            è²èª¿ TEXT,
            è¨»é‡‹ TEXT,
            å¤šéŸ³å­— TEXT
        )
    ''')
    conn.commit()

    log_lines = []
    update_rows = pd.DataFrame()  # é»˜è®¤ç©ºçš„ DataFrame

    def clean_join(series):
        return ", ".join(x.strip() for x in series.dropna().astype(str).unique() if x and x.strip())

    # åªæœ‰å½“ append=True æ—¶ï¼Œæ‰è¿›è¡Œç­›é€‰
    if append:
        try:
            df_append = pd.read_excel(APPEND_PATH, sheet_name="æª”æ¡ˆ")
            update_rows = df_append[df_append['å¾…æ›´æ–°'] == 1]
        except:
            print("è¯»å– APPEND_PATH æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡ç­›é€‰ã€‚")

        # å¦‚æœ append ä¸º Trueï¼Œåˆ é™¤æ•°æ®åº“ä¸­ä¸å¾…æ›´æ–°è¡Œä¸­â€œç°¡ç¨±â€åŒ¹é…çš„è®°å½•
        if not update_rows.empty:
            # å»é™¤ NaN å’Œç©ºå€¼ï¼Œç¡®ä¿åªæœ‰æœ‰æ•ˆçš„ç°¡ç¨±åˆ—å‚ä¸åˆ é™¤æ“ä½œ
            valid_ç°¡ç¨± = update_rows['ç°¡ç¨±'].dropna()  # å»é™¤ NaN å€¼
            for row in valid_ç°¡ç¨±:
                cursor.execute("DELETE FROM dialects WHERE ç°¡ç¨± = ?", (row,))
            conn.commit()

    for path in tsv_paths:
        if path == "_":
            continue

        # tsv_name = os.path.splitext(os.path.basename(path))[0]
        tsv_name = get_tsvs(single=path)[1][0]
        now_process = f"\nğŸ” æ­£åœ¨è™•ç†ï¼š{tsv_name}"
        print(now_process)
        with open(MISSING_DATA_LOG, "a", encoding="utf-8") as f:
            f.write("\n" + now_process + "\n")

        # å¦‚æœ append ä¸º Trueï¼Œåˆ™è¿›è¡Œç­›é€‰
        if append and not update_rows.empty and tsv_name not in update_rows['ç°¡ç¨±'].values:
            print(f"è·³éï¼š{tsv_name} (ä¸åœ¨å¾…æ›´æ–°æ¸…å–®ä¸­)")
            continue

        try:
            df = extract_all_from_files(path)
            print(f"  ğŸ“„ æå–è³‡æ–™è¡¨ï¼š{len(df)} è¡Œ")

            df = df.fillna("")
            df["æ¼¢å­—"] = df["æ±‰å­—"].astype(str).str.strip()
            df["éŸ³ç¯€"] = df["éŸ³æ ‡"].astype(str).str.strip()
            df["è²æ¯"] = df["å£°æ¯"].astype(str).str.strip()
            df["éŸ»æ¯"] = df["éŸµæ¯"].astype(str).str.strip()
            df["è²èª¿"] = df["å£°è°ƒ"].astype(str).str.strip()
            df["è¨»é‡‹"] = df["è¨»é‡‹"].astype(str).str.strip() if "è¨»é‡‹" in df.columns else ""

            insert_count = 0
            for _, row in df.iterrows():
                char = row["æ¼¢å­—"]
                phonetic = row["éŸ³ç¯€"]
                cons = row["è²æ¯"]
                vow = row["éŸ»æ¯"]
                tone = row["è²èª¿"]
                note = row["è¨»é‡‹"]

                if not any([cons, vow, tone]):
                    continue

                if not all([cons, vow, tone]):
                    log_message = f"â— ç¼ºè³‡æ–™ï¼šchar={char}, éŸ³ç¯€={phonetic}, è²æ¯='{cons}', éŸ»æ¯='{vow}', è²èª¿='{tone}'"
                    # print(log_message)
                    with open(MISSING_DATA_LOG, "a", encoding="utf-8") as f:
                        f.write(log_message + "\n")

                cursor.execute('''
                    INSERT INTO dialects (ç°¡ç¨±, æ¼¢å­—, éŸ³ç¯€, è²æ¯, éŸ»æ¯, è²èª¿, è¨»é‡‹, å¤šéŸ³å­—)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    tsv_name, char, phonetic,
                    cons, vow, tone, note, ""
                ))
                insert_count += 1

            conn.commit()
            log_lines.append(f"{tsv_name} å¯«å…¥äº† {insert_count} ç­†ã€‚")
            # print(f"âœ… {tsv_name} å®Œæˆï¼šå…±å¯«å…¥ {insert_count} ç­†ã€‚")

        except Exception as e:
            error_detail = traceback.format_exc()
            log_lines.append(f"âŒ {tsv_name} å¯«å…¥å¤±æ•—ï¼š\n{error_detail}")
            print(f"âŒ éŒ¯èª¤è™•ç† {tsv_name}ï¼š\n{error_detail}")

    conn.close()
    print(f"\nğŸ“¦ æ‰€æœ‰è³‡æ–™å·²å¯«å…¥ï¼š{db_path}")
    conn_all = sqlite3.connect(db_path)
    # å‰µå»ºç´¢å¼•ï¼ŒåŠ å¿«æŸ¥è©¢é€Ÿåº¦
    print("â€» é–‹å§‹å‰µå»ºç´¢å¼• â€»")
    conn_all.execute("CREATE INDEX IF NOT EXISTS idx_loc ON dialects(ç°¡ç¨±);")
    conn_all.execute("CREATE INDEX IF NOT EXISTS idx_char ON dialects(æ¼¢å­—);")
    conn_all.commit()

    # print("\nğŸ“Š å¯«å…¥ç¸½çµï¼š")
    # for line in log_lines:
    # print("   " + line)

    with open(WRITE_INFO_LOG, "w", encoding="utf-8") as f:
        f.write("\n".join(log_lines))
    # print(f"\nğŸ“ å·²å¯«å…¥ç´€éŒ„è‡³ï¼š{log_path}")


# èˆŠç‰ˆä»£ç¢¼ï¼Œç›´æ¥åˆªé™¤æ•´å€‹æ•¸æ“šåº«ä¸¦æ›´æ–°(å¿«ï¼Œä½†æ˜¯ç”µè„‘ä¼šç‰¹åˆ«å¡ï¼‰
def process_polyphonic_annotations(db_path: str):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query("SELECT * FROM dialects ORDER BY ç°¡ç¨±, æ¼¢å­—", conn)

    print(f"ğŸ” è³‡æ–™åº«è®€å–å®Œæˆï¼Œå…± {len(df)} ç­†")

    # ä¸€éšæ®µï¼šåˆä½µåŒéŸ³ç¯€è¨»é‡‹ï¼ˆè²æ¯ã€éŸ»æ¯ã€è²èª¿ä¸€è‡´ï¼‰
    merged = []
    grouped = df.groupby(["ç°¡ç¨±", "æ¼¢å­—", "éŸ³ç¯€"])

    previous_short_name = None  # ç”¨æ¥ä¿å­˜ä¸Šä¸€æ¬¡çš„åœ°ç‚¹ä¿¡æ¯
    count_num = 1
    for (short_name, char, syllable), group in grouped:
        if short_name != previous_short_name:  # å½“åœ°ç‚¹å˜åŒ–æ—¶è§¦å‘
            print(f"æ­£åœ¨è™•ç†ï¼š{short_name}(ç¬¬{count_num}å€‹)")  # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œåœ°ç‚¹å‘ç”Ÿå˜åŒ–
            count_num += 1

        unique_phonetics = group[["è²æ¯", "éŸ»æ¯", "è²èª¿"]].drop_duplicates()
        if len(unique_phonetics) == 1:
            notes = group["è¨»é‡‹"].dropna().astype(str).str.strip().unique()
            notes = [n for n in notes if n]
            combined_note = ";".join(notes) if notes else ""

            base_row = group.iloc[0].copy()
            # if base_row["è¨»é‡‹"] != combined_note:
            #     print(f"ğŸ“ åˆä½µè¨»é‡‹ï¼š{char} / {syllable} â†’ ã€Œ{combined_note}ã€")
            base_row["è¨»é‡‹"] = combined_note
            merged.append(base_row)
        else:
            print(f"âš ï¸ éŸ³ç¯€ç›¸åŒä½†è²éŸ»èª¿ä¸åŒï¼š{char} / {syllable}")
            for _, row in group.iterrows():
                merged.append(row)
        previous_short_name = short_name  # æ›´æ–°ä¹‹å‰çš„åœ°ç‚¹

    merged_df = pd.DataFrame(merged)
    print(f"âœ… åˆä½µå¾Œå‰©é¤˜ {len(merged_df)} ç­†")

    # äºŒéšæ®µï¼šæ¨™è¨˜å¤šéŸ³å­—ï¼ˆéŸ³ç¯€ä¸åŒï¼‰
    # final = []
    grouped2 = merged_df.groupby(["ç°¡ç¨±", "æ¼¢å­—"])

    # for (short_name, char), group in grouped2:
    #     if len(group["éŸ³ç¯€"].unique()) > 1:
    #         # print(f"ğŸ” å¤šéŸ³å­—æ¨™è¨˜ï¼š{short_name} / {char}")
    #         group["å¤šéŸ³å­—"] = "1"
    #         # for _, row in group.iterrows():
    #         # print("  â¤", dict(row))
    #     else:
    #         group["å¤šéŸ³å­—"] = ""
    #     final.append(group)
    # final_df = pd.concat(final).reset_index(drop=True)

    # ä½¿ç”¨ `transform()` åˆ¤æ–­æ˜¯å¦å¤šéŸ³å­—
    merged_df['å¤šéŸ³å­—'] = grouped2['éŸ³ç¯€'].transform(lambda x: '1' if x.nunique() > 1 else '')
    final_df = merged_df

    # print(f"ğŸ’¾ æ¸…ç©ºä¸¦é‡å»ºè³‡æ–™è¡¨ dialectsï¼Œå…± {len(final_df)} ç­†")
    cursor = conn.cursor()
    cursor.execute("DROP TABLE IF EXISTS dialects")
    final_df.to_sql("dialects", conn, index=False)

    conn.commit()
    conn.close()
    print("âœ… å¤šéŸ³å­—è™•ç†å®Œæˆ")


# æ–°ä»£ç¢¼ï¼Œå¯¦æ™‚æ›´æ”¹æ•¸æ“šåº«ï¼ŒåŠ å¿«é‹è¡Œé€Ÿåº¦(å®é™…ä¸Šè¿˜å˜æ…¢äº†ã€‚ã€‚ï¼‰
def process_polyphonic_annotations_new(db_path: str, append: bool = False):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query("SELECT * FROM dialects ORDER BY ç°¡ç¨±, æ¼¢å­—", conn)
    # å¦‚æœ append æ¨¡å¼é–‹å•Ÿï¼Œåªä¿ç•™æŒ‡å®šç°¡ç¨±
    if append:
        try:
            df_append = pd.read_excel(APPEND_PATH, sheet_name="æª”æ¡ˆ")
            update_rows = df_append[df_append['å¾…æ›´æ–°'] == 1]
            valid_ç°¡ç¨± = update_rows['ç°¡ç¨±'].dropna().unique().tolist()

            if valid_ç°¡ç¨±:
                df = df[df["ç°¡ç¨±"].isin(valid_ç°¡ç¨±)]
                print(f"ğŸ“Œ åªè™•ç†å¾…æ›´æ–°ç°¡ç¨±ï¼š{valid_ç°¡ç¨±}")
            else:
                print("âš ï¸ APPEND_PATH ä¸­æœªç™¼ç¾ä»»ä½•å¾…æ›´æ–°ç°¡ç¨±ï¼Œè·³éè™•ç†ã€‚")
                conn.close()
                return
        except Exception as e:
            print(f"â— ç„¡æ³•è®€å– APPEND_PATHï¼š{e}ï¼Œå°‡è™•ç†å…¨éƒ¨è³‡æ–™ã€‚")

    print(f"ğŸ” å¾…è™•ç†è³‡æ–™ç­†æ•¸ï¼š{len(df)}")

    grouped = df.groupby(["ç°¡ç¨±", "æ¼¢å­—"])

    previous_short_name = None  # ç”¨æ¥ä¿å­˜ä¸Šä¸€æ¬¡çš„åœ°ç‚¹ä¿¡æ¯
    count_num = 1
    for (short_name, char), group in grouped:
        if short_name != previous_short_name:  # å½“åœ°ç‚¹å˜åŒ–æ—¶è§¦å‘
            print(f"æ­£åœ¨è™•ç†ï¼š{short_name}(ç¬¬{count_num}å€‹)")  # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œåœ°ç‚¹å‘ç”Ÿå˜åŒ–
            count_num += 1

        # ä¸€éšæ®µï¼šè™•ç†è¨»é‡‹
        grouped_syllables = group.groupby("éŸ³ç¯€")
        for syllable, syllable_group in grouped_syllables:
            unique_phonetics = syllable_group[["è²æ¯", "éŸ»æ¯", "è²èª¿"]].drop_duplicates()
            if len(unique_phonetics) == 1:
                notes = syllable_group["è¨»é‡‹"].dropna().astype(str).str.strip().unique()
                notes = [n for n in notes if n]
                combined_note = ";".join(notes) if notes else ""

                base_row = syllable_group.iloc[0].copy()
                base_row["è¨»é‡‹"] = combined_note

                # æ›´æ–°è³‡æ–™åº«ä¸­çš„è¨»é‡‹
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE dialects
                    SET è¨»é‡‹ = ?
                    WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                """, (combined_note, short_name, char, syllable))

                # åˆªé™¤é™¤ç¬¬ä¸€è¡Œå¤–çš„å…¶ä»–é‡è¤‡è¡Œ
                cursor.execute("""
                    DELETE FROM dialects
                    WHERE (ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?)
                      AND rowid NOT IN (
                        SELECT MIN(rowid)
                        FROM dialects
                        WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                      )
                """, (short_name, char, syllable, short_name, char, syllable))

            else:
                print(f"âš ï¸ éŸ³ç¯€ç›¸åŒä½†è²éŸ»èª¿ä¸åŒï¼š{char} / {syllable}")
                for _, row in syllable_group.iterrows():
                    cursor = conn.cursor()
                    cursor.execute("""
                        UPDATE dialects
                        SET è¨»é‡‹ = ?
                        WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                    """, (row["è¨»é‡‹"], short_name, char, syllable))

        # äºŒéšæ®µï¼šæ¨™è¨˜å¤šéŸ³å­—ï¼ˆéŸ³ç¯€ä¸åŒï¼‰
        syllables_count = len(group["éŸ³ç¯€"].unique())
        if syllables_count > 1:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE dialects
                SET å¤šéŸ³å­— = '1'
                WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ?
            """, (short_name, char))

        previous_short_name = short_name  # æ›´æ–°ä¹‹å‰çš„åœ°ç‚¹

    conn.commit()
    print("ğŸ”¨ è³‡æ–™åº«æ›´æ–°å®Œæˆï¼")

    conn.close()


def sync_dialects_flags(all_db_path=DIALECTS_DB_PATH,
                        query_db_path=QUERY_DB_PATH,
                        log_path=CHARACTERS_DB_PATH):
    # è®€å– dialects_all.db ä¸­æ‰€æœ‰å”¯ä¸€ç°¡ç¨±
    conn_all = sqlite3.connect(all_db_path)
    # å‰µå»ºç´¢å¼•ï¼ŒåŠ å¿«æŸ¥è©¢é€Ÿåº¦
    print("â€» é–‹å§‹å‰µå»ºç´¢å¼• â€»")
    conn_all.execute("CREATE INDEX IF NOT EXISTS idx_loc ON dialects(ç°¡ç¨±);")
    conn_all.execute("CREATE INDEX IF NOT EXISTS idx_char ON dialects(æ¼¢å­—);")
    conn_all.commit()
    cursor_all = conn_all.cursor()
    cursor_all.execute("SELECT DISTINCT ç°¡ç¨± FROM dialects")
    all_tags = set(row[0] for row in cursor_all.fetchall())
    conn_all.close()

    # è®€å– dialects_query.db ä¸­æ‰€æœ‰ç°¡ç¨±
    conn_query = sqlite3.connect(query_db_path)
    cursor_query = conn_query.cursor()

    # ç¢ºä¿å­˜å„²æ¨™è¨˜æ¬„ä½å­˜åœ¨
    cursor_query.execute("PRAGMA table_info(dialects)")
    columns = [col[1] for col in cursor_query.fetchall()]
    if "å­˜å„²æ¨™è¨˜" not in columns:
        cursor_query.execute("ALTER TABLE dialects ADD COLUMN å­˜å„²æ¨™è¨˜ INTEGER DEFAULT 0")

    cursor_query.execute("SELECT rowid, ç°¡ç¨± FROM dialects")
    query_map = {tag: rowid for rowid, tag in cursor_query.fetchall()}

    matched = []
    unmatched = []

    for tag in sorted(all_tags):
        if tag in query_map:
            rowid = query_map[tag]
            cursor_query.execute("UPDATE dialects SET å­˜å„²æ¨™è¨˜ = 1 WHERE rowid = ?", (rowid,))
            matched.append(tag)
        else:
            unmatched.append(tag)
            print(f"â— ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{tag}")

    conn_query.commit()
    conn_query.close()

    # å¯«å…¥ log æª”æ¡ˆï¼ˆå‰é¢å…©å€‹ç©ºè¡Œï¼‰
    with open(log_path, "a", encoding="utf-8") as f:
        f.write("\n\n")
        for tag in unmatched:
            f.write(f"ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{tag}\n")

        # å¯«å…¥æˆåŠŸå­˜å„²è¨Šæ¯ï¼Œæ¯ 10 å€‹æ›è¡Œ
        lines = []
        for i in range(0, len(matched), 10):
            lines.append(", ".join(matched[i:i + 10]))
        success_message = "æˆåŠŸå­˜å„²ï¼š\n" + "\n".join(lines)
        f.write(success_message + "\n")

    print("âœ… åŒæ­¥å®Œæˆã€‚å·²æ›´æ–°å­˜å„²æ¨™è¨˜ã€‚")


def process_phonology_excel(
        excel_file=PHO_TABLE_PATH,
        sheet_name="å±¤ç´š",
        db_file=CHARACTERS_DB_PATH,
        log_file=WRITE_INFO_LOG
):
    os.makedirs("data", exist_ok=True)

    # æ¬„ä½è¨­ç½®
    columns_needed = ["æ”", "å‘¼", "ç­‰", "éŸ»", "å…¥", "èª¿", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "éƒ¨ä½", "æ–¹å¼", "å–®å­—", "é‡‹ç¾©"]
    rename_map = {"å–®å­—": "æ¼¢å­—"}
    write_columns = ["æ”", "å‘¼", "ç­‰", "éŸ»", "å…¥", "èª¿", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "éƒ¨ä½", "æ–¹å¼", "æ¼¢å­—", "é‡‹ç¾©"]

    # è®€å– Excel
    try:
        df = pd.read_excel(excel_file, sheet_name=sheet_name, dtype=str)
    except Exception as e:
        print(f"âŒ è®€å– Excel å¤±æ•—: {e}")
        return

    try:
        df = df[columns_needed].rename(columns=rename_map)
    except KeyError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {e}")
        return

    # æ¸…é™¤æ¼¢å­—ç‚ºç©ºçš„è¡Œ
    df = df[df["æ¼¢å­—"].notna() & (df["æ¼¢å­—"].str.strip() != "")]
    df['num'] = df.index + 2  # Excel è¡Œè™Ÿ

    # æª¢æŸ¥å…¶ä»–æ¬„ä½æ˜¯å¦æœ‰ç¼ºå€¼ï¼ˆä¸åŒ…å«"æ¼¢å­—"èˆ‡"num"ï¼‰
    check_cols = [col for col in df.columns if col not in ["æ¼¢å­—", "num", "é‡‹ç¾©"]]
    invalid_rows = df[df[check_cols].isnull().any(axis=1)]

    # æœ‰æ•ˆåˆ—
    df_valid = df.drop(index=invalid_rows.index)

    # å»é™¤å®Œå…¨é‡è¤‡çš„åˆ—ï¼ˆåªæ¯”è¼ƒè¦å¯«å…¥çš„åˆ—ï¼‰
    df_unique = df_valid.drop_duplicates(subset=write_columns).copy()

    # æ¨™è¨˜ã€Œå¤šåœ°ä½ã€ï¼šåŒæ¼¢å­—å‡ºç¾å¤šæ¬¡ï¼ˆä½†è¡Œä¸åŒï¼‰
    dup_counts = df_unique["æ¼¢å­—"].value_counts()
    df_unique["å¤šåœ°ä½æ¨™è¨˜"] = df_unique["æ¼¢å­—"].map(lambda x: "1" if dup_counts.get(x, 0) > 1 else "")

    # è¼¸å‡ºéŒ¯èª¤è¨˜éŒ„
    if not invalid_rows.empty:
        invalid_output = invalid_rows[["num", "æ¼¢å­—"] + check_cols]
        print("â— ç™¼ç¾æ¬„ä½ç¼ºæ¼å¦‚ä¸‹ï¼š")
        print(invalid_output)

        with open(log_file, "a", encoding="utf-8") as f:
            f.write(invalid_output.to_csv(index=False, sep='\t', lineterminator='\n'))

    # å¯«å…¥ SQLite
    try:
        conn = sqlite3.connect(db_file)
        df_unique.drop(columns=["num"]).to_sql("characters", conn, if_exists="replace", index=False)
        # â¤ å»ºç«‹ç´¢å¼•
        index_columns = [col for col in write_columns if col != "é‡‹ç¾©"]  # æ’é™¤ã€Œé‡‹ç¾©ã€
        index_columns.append("å¤šåœ°ä½æ¨™è¨˜")
        for col in index_columns:
            index_name = f"idx_characters_{col}"
            conn.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON characters({col});")

        conn.close()
        print("âœ… æˆåŠŸå¯«å…¥ SQLiteï¼Œç¸½ç­†æ•¸ï¼š", len(df_unique))
    except Exception as e:
        print(f"âŒ SQLite å¯«å…¥å¤±æ•—: {e}")


def write_to_sql(yindian=None, write_chars_db=None, append=False):
    #  å¯«æª”æ¡ˆè¡¨
    print("å¼€å§‹å¯«å…¥æª”æ¡ˆè¡¨")
    build_dialect_database()

    #  å¯«ç¸½æ•¸æ“šè¡¨
    if yindian:
        if yindian == 'only':
            tsv_paths_yindian, *_ = get_tsvs(output_dir=YINDIAN_DATA_DIR)
            tsv_paths = [
                p for p in tsv_paths_yindian
                if os.path.splitext(os.path.basename(p))[0] not in exclude_files
            ]
        else:
            tsv_paths_yindian, *_ = get_tsvs(output_dir=YINDIAN_DATA_DIR)
            tsv_paths_mine, *_ = get_tsvs()
            # ç”¨å­—å…¸æ¥ä¿å­˜æœ€ç»ˆçš„è·¯å¾„ï¼Œå¹¶æŒ‰æ–‡ä»¶åè¿›è¡Œåˆå¹¶
            merged_paths = {}
            # å°† tsv_paths_yindian ä¸­çš„æ–‡ä»¶è·¯å¾„æ·»åŠ åˆ°å­—å…¸ä¸­ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºé”®
            for path in tsv_paths_yindian:
                filename = os.path.basename(path)
                merged_paths[filename] = path
            # éå† tsv_paths_mineï¼Œå¦‚æœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œæ›´æ–°ä¸º mine ä¸­çš„è·¯å¾„
            for path in tsv_paths_mine:
                filename = os.path.basename(path)
                merged_paths[filename] = path  # ç›´æ¥è¦†ç›–å·²æœ‰è·¯å¾„
            # print(merged_paths)
            # åˆå¹¶å®Œæˆåçš„è·¯å¾„åˆ—è¡¨
            # tsv_paths = list(merged_paths.values())
            tsv_paths = [
                path for file, path in merged_paths.items()
                if os.path.splitext(file)[0] not in exclude_files
            ]
            # print(tsv_paths)
            # tsv_paths = tsv_paths_yindian + tsv_paths_mine
    else:
        tsv_paths, *_ = get_tsvs()
    db_path = os.path.join(os.getcwd(), DIALECTS_DB_PATH)
    print("ğŸš€ é–‹å§‹å°å…¥è³‡æ–™...")
    process_all2sql(tsv_paths, db_path, append)
    print("å¼€å§‹å¤„ç†é‡å¤è¡Œä»¥åŠæ ‡è®°å¤šéŸ³å­—")
    if append:
        process_polyphonic_annotations_new(DIALECTS_DB_PATH, append=True)
    else:
        process_polyphonic_annotations(DIALECTS_DB_PATH)
    print("å¼€å§‹å¯«å…¥å­˜å„²æ¨™è¨˜")
    sync_dialects_flags()

    if write_chars_db:
        #  å¯«æ¼¢å­—åœ°ä½è¡¨
        print("å¼€å§‹å¯«å…¥æ¼¢å­—åœ°ä½è¡¨")
        process_phonology_excel()
    # print("âœ… æ¸¬è©¦å®Œæˆã€‚")


if __name__ == "__main__":
    write_to_sql()
    # build_dialect_database()
