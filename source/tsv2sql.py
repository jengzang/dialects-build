import os
import re
import sqlite3
import traceback
import time
from pathlib import Path

import pandas as pd

from common.constants import exclude_files
from source.change_coordinates import GPSUtil
from common.config import HAN_PATH, APPEND_PATH, QUERY_DB_PATH, DIALECTS_DB_PATH, CHARACTERS_DB_PATH, PHO_TABLE_PATH, \
    MISSING_DATA_LOG, WRITE_INFO_LOG, YINDIAN_DATA_DIR, UPDATE_DATA_DIR
from source.get_new import extract_all_from_files
from source.match_fromdb import get_tsvs
from common.config import (QUERY_DB_ADMIN_PATH, QUERY_DB_USER_PATH,
                           DIALECTS_DB_ADMIN_PATH, DIALECTS_DB_USER_PATH)

def build_dialect_database(mode='admin'):
    """
    æ§‹å»ºæ–¹è¨€æŸ¥è©¢æ•¸æ“šåº«

    Args:
        mode: 'admin' æˆ– 'user'

    Returns:
        list: TSV è·¯å¾‘åˆ—è¡¨ï¼ˆç”¨æ–¼å¾ŒçºŒå¯«å…¥æ•¸æ“šï¼‰
    """
    from common.config import (QUERY_DB_ADMIN_PATH, QUERY_DB_USER_PATH,
                               HAN_PATH, APPEND_PATH)
    from source.match_fromdb import scan_tsv_with_conflict_resolution
    from common.s2t import simplified2traditional, traditional2simplified

    # 1. ç¢ºå®šæ•¸æ“šåº«è·¯å¾‘
    if mode == 'admin':
        sqlite_db = Path(QUERY_DB_ADMIN_PATH)
    else:  # user
        sqlite_db = Path(QUERY_DB_USER_PATH)

    print(f"\n æ§‹å»º {mode} æ¨¡å¼æ•¸æ“šåº«ï¼š{sqlite_db}")

    han_file = Path(HAN_PATH)
    other_file = Path(APPEND_PATH)

    # --- æ¬„ä½å°æ‡‰ ---
    tone_map = {
        "[1]é™°å¹³": "T1é™°å¹³",
        "[2]é™½å¹³": "T2é™½å¹³",
        "[3]é™°ä¸Š": "T3é™°ä¸Š",
        "[4]é™½ä¸Š": "T4é™½ä¸Š",
        "[5]é™°å»": "T5é™°å»",
        "[6]é™½å»": "T6é™½å»",
        "[7]é™°å…¥": "T7é™°å…¥",
        "[8]é™½å…¥": "T8é™½å…¥",
        "[9]è®Šèª¿": "T9å…¶ä»–èª¿",
        "[10]è¼•è²": "T10è¼•è²"
    }

    geo_map = {
        "çœ/è‡ªæ²»å€/ç›´è½„å¸‚": "çœ",
        "åœ°å€/å¸‚/å·": "å¸‚",
        "ç¸£/å¸‚/å€": "ç¸£",
        "é„‰/é®/è¡—é“": "é®",
        "æ‘/ç¤¾å€/å±…æ°‘é»": "è¡Œæ”¿æ‘",
        "è‡ªç„¶æ‘": "è‡ªç„¶æ‘"
    }

    rename_map = {**tone_map, **geo_map}

    # æ¬„ä½æ¸…å–®ï¼ˆåŸå§‹åç¨±ï¼‰
    required_columns = [
        "èªè¨€", "ç°¡ç¨±", "éŸ³å…¸æ’åº", "åœ°åœ–é›†äºŒåˆ†å€", "éŸ³å…¸åˆ†å€", "å­—è¡¨ä¾†æºï¼ˆæ¯æœ¬ï¼‰", "æ–¹è¨€å³¶",
        "å­˜å„²æ¨™è¨˜", "ç¶“ç·¯åº¦", "åœ°åœ–ç´šåˆ¥",
        *geo_map.keys(),
        *tone_map.keys(),
        "isUser"  # æ·»åŠ  isUser åˆ—
    ]

    # --- è®€å– Append_files.xlsx ---
    df_other = pd.read_excel(other_file, sheet_name="æª”æ¡ˆ", header=0)
    df_other.columns = df_other.columns.str.strip()
    df_other["å­˜å„²æ¨™è¨˜"] = ""  #  è£œä¸Šé€™ä¸€åˆ—
    df_other = df_other[[col for col in required_columns if col in df_other.columns]].copy()
    df_other = df_other.rename(columns=rename_map)

    # --- è®€å– æ¼¢å­—éŸ³å…¸è¡¨ï¼Œè·³éç¬¬ 2 è¡Œï¼ˆå³ index 0ï¼‰---
    df_han = pd.read_excel(han_file, sheet_name="æª”æ¡ˆ", header=0, engine='openpyxl', keep_default_na=False)
    df_han = df_han.drop(index=0).reset_index(drop=True)
    df_han.columns = df_han.columns.str.strip()
    df_han["å­˜å„²æ¨™è¨˜"] = ""  #  è£œä¸Šé€™ä¸€åˆ—
    df_han = df_han[[col for col in required_columns if col in df_han.columns]].copy()
    df_han = df_han.rename(columns=rename_map)

    # --- è™•ç†ç¶“ç·¯åº¦è½‰æ› ---
    def convert_coordinates(df):
        """
        å° 'ç¶“ç·¯åº¦' åˆ—é€²è¡Œåæ¨™è½‰æ›ï¼šBD-09 (ç™¾åº¦) â†’ WGS-84 (GPS)
        """
        new_coordinates = []
        for coords in df['ç¶“ç·¯åº¦']:
            # å¦‚æœç¶“ç·¯åº¦ç‚ºç©ºï¼Œè·³é
            if pd.isna(coords) or coords.strip() == '':
                new_coordinates.append(None)  # å¦‚æœæ˜¯ç©ºå€¼ï¼Œå°‡ç¶“ç·¯åº¦è¨­ç‚º None
                continue

            # ç¢ºä¿ coords æ˜¯å­—ç¬¦ä¸²é¡å‹
            coords = str(coords).strip()

            # åˆ†å‰²ç¶“ç·¯åº¦ï¼ˆæ ¼å¼ï¼šç¶“åº¦,ç·¯åº¦ï¼‰
            bd_lon, bd_lat = map(float, re.split(r'[ï¼Œ,]', coords))

            # BD-09 â†’ WGS-84 è½‰æ›ï¼ˆæ³¨æ„ï¼šGPSUtil åƒæ•¸é †åºæ˜¯ lat, lonï¼‰
            wgs_lat, wgs_lon = GPSUtil.bd09_to_gps84(bd_lat, bd_lon)
            new_coordinates.append(f"{wgs_lon},{wgs_lat}")  # å­˜å„²æ ¼å¼ï¼šç¶“åº¦,ç·¯åº¦

        # æ›´æ–° 'ç¶“ç·¯åº¦' åˆ—
        df['ç¶“ç·¯åº¦'] = new_coordinates
        return df

    # è™•ç† df_other å’Œ df_han å…©å€‹ DataFrame
    df_other = convert_coordinates(df_other)
    df_han = convert_coordinates(df_han)

    # 2. è®€å–å…©å€‹ Excel æ–‡ä»¶
    print("\nâ³ è®€å–å…ƒæ•¸æ“šæ–‡ä»¶...")
    print(f"   HAN_PATH: {len(df_han)} å€‹æ–¹è¨€é»")
    print(f"   APPEND_PATH: {len(df_other)} å€‹æ–¹è¨€é»")

    # 3. æƒæ TSV æ–‡ä»¶ä¸¦è™•ç†è¡çªï¼ˆä¸ä¾è³´æ•¸æ“šåº«ï¼‰
    print(f"\nâ³ æƒæ TSV æ–‡ä»¶ï¼ˆ{mode} æ¨¡å¼ï¼‰...")
    print(f"   æ­£åœ¨æƒæ yindian å’Œ processed ç›®éŒ„...")
    tsv_paths, sources = scan_tsv_with_conflict_resolution(mode=mode, append_df=df_other)

    print(f"\nâœ… æœ€çµ‚ç¢ºå®š {len(tsv_paths)} å€‹ TSV æ–‡ä»¶")

    # 4. æ ¹æ“š TSV ä¾†æºé¸æ“‡å…ƒæ•¸æ“š
    print(f"\nâ³ æ ¹æ“š TSV ä¾†æºé¸æ“‡å…ƒæ•¸æ“š...")
    # å»ºç«‹ ç°¡ç¨± -> TSVä¾†æº çš„æ˜ å°„ï¼ˆè™•ç†ç¹ç°¡è½‰æ›ï¼‰
    tsv_name_to_source = {}
    for filename, source in sources.items():
        variants = [filename]
        try:
            variants.append(simplified2traditional(filename))
        except:
            pass
        try:
            variants.append(traditional2simplified(filename))
        except:
            pass

        for variant in variants:
            tsv_name_to_source[variant] = source

    print(f"   å»ºç«‹äº† {len(tsv_name_to_source)} å€‹ç°¡ç¨±æ˜ å°„")
    print(f"\nâ³ åŒ¹é…å…ƒæ•¸æ“šèˆ‡ TSV æ–‡ä»¶...")

    final_rows = []
    all_abbr = set(df_han['ç°¡ç¨±'].tolist() + df_other['ç°¡ç¨±'].tolist())
    print(f"   å…±æœ‰ {len(all_abbr)} å€‹å”¯ä¸€ç°¡ç¨±éœ€è¦è™•ç†")

    # User æ¨¡å¼ï¼šåªä¿ç•™ isUser=1 çš„ç°¡ç¨±
    if mode == 'user':
        print(f"\n User æ¨¡å¼ï¼šéæ¿¾ isUser=1 çš„ç°¡ç¨±...")
        if 'isUser' in df_other.columns:
            user_abbr = set(df_other[df_other['isUser'] == 1]['ç°¡ç¨±'].tolist())
            # ä¿ç•™ HAN ä¸­çš„æ‰€æœ‰ç°¡ç¨± + APPEND ä¸­ isUser=1 çš„ç°¡ç¨±
            han_abbr = set(df_han['ç°¡ç¨±'].tolist())
            all_abbr = han_abbr | user_abbr
            print(f"   HAN ç°¡ç¨±: {len(han_abbr)} å€‹")
            print(f"   APPEND isUser=1 ç°¡ç¨±: {len(user_abbr)} å€‹")
            print(f"   åˆä½µå¾Œ: {len(all_abbr)} å€‹")
        else:
            print(f"   è­¦å‘Šï¼šAPPEND_PATH ä¸­æ²’æœ‰ isUser åˆ—ï¼Œä½¿ç”¨æ‰€æœ‰ç°¡ç¨±")

    for idx, abbr in enumerate(all_abbr, 1):
        # æ¯è™•ç† 100 å€‹ç°¡ç¨±æ‰“å°ä¸€æ¬¡é€²åº¦
        if idx % 100 == 0 or idx == len(all_abbr):
            print(f"   è™•ç†é€²åº¦: {idx}/{len(all_abbr)}")

        # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„ TSV æ–‡ä»¶
        source = tsv_name_to_source.get(abbr)

        # æ ¹æ“š TSV ä¾†æºé¸æ“‡å…ƒæ•¸æ“š
        if source == 'yindian':
            # æœ‰ yindian TSVï¼šå„ªå…ˆä½¿ç”¨ HAN_PATH
            rows_han = df_han[df_han['ç°¡ç¨±'] == abbr]
            if not rows_han.empty:
                selected_row = rows_han.iloc[0]
            else:
                # HAN ä¸­æ²’æœ‰ï¼Œå˜—è©¦ APPEND
                rows_other = df_other[df_other['ç°¡ç¨±'] == abbr]
                if not rows_other.empty:
                    selected_row = rows_other.iloc[0]
                else:
                    continue

        elif source == 'processed':
            # æœ‰ processed TSVï¼šå„ªå…ˆä½¿ç”¨ APPEND_PATH
            rows_other = df_other[df_other['ç°¡ç¨±'] == abbr]
            if not rows_other.empty:
                selected_row = rows_other.iloc[0]
            else:
                # APPEND ä¸­æ²’æœ‰ï¼Œå˜—è©¦ HAN
                rows_han = df_han[df_han['ç°¡ç¨±'] == abbr]
                if not rows_han.empty:
                    selected_row = rows_han.iloc[0]
                else:
                    continue

        else:
            # æ²’æœ‰ TSV æ–‡ä»¶ï¼šå„ªå…ˆä½¿ç”¨ HAN_PATHï¼Œå¦‚æœ HAN æ²’æœ‰å‰‡ä½¿ç”¨ APPEND
            rows_han = df_han[df_han['ç°¡ç¨±'] == abbr]
            if not rows_han.empty:
                selected_row = rows_han.iloc[0]
            else:
                rows_other = df_other[df_other['ç°¡ç¨±'] == abbr]
                if not rows_other.empty:
                    selected_row = rows_other.iloc[0]
                else:
                    continue

        final_rows.append(selected_row)

    # 5. å»ºç«‹æœ€çµ‚ DataFrame
    print(f"\nâ³ å»ºç«‹æœ€çµ‚ DataFrameï¼ˆå…± {len(final_rows)} å€‹æ–¹è¨€é»ï¼‰...")
    final_df = pd.DataFrame(final_rows)

    # 6. æ‡‰ç”¨åœ°åœ–é›†äºŒåˆ†å€æ›¿æ›é‚è¼¯
    print(f"â³ æ‡‰ç”¨åœ°åœ–é›†äºŒåˆ†å€æ›¿æ›é‚è¼¯...")
    def replace_dialect_zone(val):
        if isinstance(val, str):
            if val.startswith("å®¢å®¶è©±-ç²µåŒ—ç‰‡"):
                return val.replace("å®¢å®¶è©±-ç²µåŒ—ç‰‡", "å®¢å®¶è©±-ç²µåŒ—ç‰‡Â·å®¢", 1)
            elif val.startswith("å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡"):
                return val.replace("å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡", "å¹³è©±å’ŒåœŸè©±-ç²µåŒ—ç‰‡Â·åœŸ", 1)
        return val

    final_df["åœ°åœ–é›†äºŒåˆ†å€"] = final_df["åœ°åœ–é›†äºŒåˆ†å€"].apply(replace_dialect_zone)

    # 7. æ’åº
    print(f"â³ æŒ‰éŸ³å…¸æ’åºæ’åº...")
    final_df = final_df.sort_values(by="éŸ³å…¸æ’åº", na_position="last")

    # 8. å¯«å…¥ SQLite
    print(f"â³ å¯«å…¥ SQLite æ•¸æ“šåº«...")
    with sqlite3.connect(sqlite_db) as conn:
        # å¯«å…¥è³‡æ–™åº«
        final_df.to_sql("dialects", conn, if_exists="replace", index=False)
        print(f"â³ å‰µå»ºç´¢å¼•...")
        # åŠ ç´¢å¼•
        conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_dialects_code ON dialects(ç°¡ç¨±);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_yindian_zone ON dialects(éŸ³å…¸åˆ†å€);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_atlas_zone ON dialects(åœ°åœ–é›†äºŒåˆ†å€);")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_flag ON dialects(å­˜å„²æ¨™è¨˜);")
        # æ–°å¢ï¼šè¤‡åˆç´¢å¼•ï¼Œå„ªåŒ–å¸¸è¦‹æŸ¥è©¢ WHERE ç°¡ç¨± = ? AND å­˜å„²æ¨™è¨˜ = ?
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_code_flag ON dialects(ç°¡ç¨±, å­˜å„²æ¨™è¨˜);")
        # ğŸš€ ã€ä¼˜å…ˆçº§é«˜ã€‘ç”¨äº match_input_tip.py çš„å­˜å‚¨æ ‡è®°è¿‡æ»¤
        conn.execute("CREATE INDEX IF NOT EXISTS idx_dialects_storage ON dialects(å­˜å„²æ¨™è¨˜, ç°¡ç¨±);")
        #  å„ªåŒ–ï¼šéŸ³å…¸åˆ†å€+å­˜å„²æ¨™è¨˜è¤‡åˆç´¢å¼•ï¼ˆç”¨æ–¼æ¨¡ç³ŠåŒ¹é…æŸ¥è©¢ï¼‰
        conn.execute("CREATE INDEX IF NOT EXISTS idx_query_partition_storage ON dialects(éŸ³å…¸åˆ†å€, å­˜å„²æ¨™è¨˜);")
        # å„ªåŒ–ï¼šåœ°åœ–é›†åˆ†å€+å­˜å„²æ¨™è¨˜è¤‡åˆç´¢å¼•ï¼ˆç”¨æ–¼match_input_tip.pyï¼‰
        conn.execute("CREATE INDEX IF NOT EXISTS idx_query_atlas_storage ON dialects(åœ°åœ–é›†äºŒåˆ†å€, å­˜å„²æ¨™è¨˜);")

    print(f"âœ… SQLite è³‡æ–™åº«å·²å»ºç«‹ï¼Œdialects è¡¨å·²æ›´æ–°å®Œæˆã€‚")

    # è¿”å› TSV è·¯å¾‘åˆ—è¡¨ï¼ˆç”¨æ–¼ write_to_sqlï¼‰
    return tsv_paths


def process_all2sql(tsv_paths, db_path, append=False, update=False, query_db_path=None):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    #  ä¼˜åŒ–ï¼šè®¾ç½® SQLite æ€§èƒ½å‚æ•°
    cursor.execute("PRAGMA synchronous = OFF")  # å…³é—­åŒæ­¥å†™å…¥
    cursor.execute("PRAGMA journal_mode = MEMORY")  # ä½¿ç”¨å†…å­˜æ—¥å¿—
    cursor.execute("PRAGMA temp_store = MEMORY")  # ä¸´æ—¶æ•°æ®å­˜å†…å­˜

    if not append and not update:  # MODIFIED: Don't drop if update mode
        cursor.execute("DROP TABLE IF EXISTS dialects")
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS dialects (
            ç°¡ç¨± TEXT,
            æ¼¢å­— TEXT,
            éŸ³ç¯€ TEXT,
            è²æ¯ TEXT,
            éŸ»æ¯ TEXT,
            è²èª¿ TEXT,
            è¨»é‡‹ TEXT,
            å¤šéŸ³å­— TEXT
        )
    ''')
    conn.commit()

    log_lines = []
    update_ç°¡ç¨±_list = []  # Track which ç°¡ç¨± to update
    processed_ç°¡ç¨± = []  # Track which ç°¡ç¨± were actually processed
    missing_data_logs = []  # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†ç¼ºå¤±æ•°æ®æ—¥å¿—

    def clean_join(series):
        return ", ".join(x.strip() for x in series.dropna().astype(str).unique() if x and x.strip())

    # åªæœ‰å½“ append=True æ—¶ï¼Œæ‰è¿›è¡Œç­›é€‰
    if append:
        try:
            df_append = pd.read_excel(APPEND_PATH, sheet_name="æª”æ¡ˆ")
            update_rows = df_append[df_append['å¾…æ›´æ–°'] == 1]
            update_ç°¡ç¨±_list = update_rows['ç°¡ç¨±'].dropna().unique().tolist()
        except:
            print("è¯»å– APPEND_PATH æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡ç­›é€‰ã€‚")

    elif update:
        # NEW: For update mode, extract ç°¡ç¨± from TSV filenames
        print(f"ğŸ“Œ update æ¨¡å¼ï¼šæ­£åœ¨æå–å¾…æ›´æ–°çš„æ–¹è¨€é»...")
        for path in tsv_paths:
            try:
                tsv_result = get_tsvs(single=path, query_db_path=query_db_path)
                if tsv_result and len(tsv_result) >= 2 and tsv_result[1]:
                    tsv_name = tsv_result[1][0]
                    if tsv_name not in update_ç°¡ç¨±_list:
                        update_ç°¡ç¨±_list.append(tsv_name)
            except:
                continue

        print(f"ğŸ“Œ update æ¨¡å¼ï¼šå°‡æ›´æ–° {len(update_ç°¡ç¨±_list)} å€‹æ–¹è¨€é»")
        print(f"   ç°¡ç¨±åˆ—è¡¨: {update_ç°¡ç¨±_list}")

    # å¦‚æœ append ä¸º Trueï¼Œåˆ é™¤æ•°æ®åº“ä¸­ä¸å¾…æ›´æ–°è¡Œä¸­"ç°¡ç¨±"åŒ¹é…çš„è®°å½•
    if (append or update) and update_ç°¡ç¨±_list:
        for ç°¡ç¨± in update_ç°¡ç¨±_list:
            cursor.execute("DELETE FROM dialects WHERE ç°¡ç¨± = ?", (ç°¡ç¨±,))
        conn.commit()
        print(f"âœ… å·²åˆªé™¤ {len(update_ç°¡ç¨±_list)} å€‹æ–¹è¨€é»çš„èˆŠæ•¸æ“š")

    for idx, path in enumerate(tsv_paths, 1):
        if path == "_":
            continue

        # ç²å– TSV æ–‡ä»¶çš„ç°¡ç¨±
        try:
            tsv_result = get_tsvs(single=path, query_db_path=query_db_path)
            if tsv_result is None or len(tsv_result) < 2 or not tsv_result[1]:
                # ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼Œè·³éè©²æ–‡ä»¶
                print(f"\n [{idx}/{len(tsv_paths)}] [è·³é] ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{os.path.basename(path)}")
                continue
            tsv_name = tsv_result[1][0]
        except (IndexError, TypeError) as e:
            # ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼Œè·³éè©²æ–‡ä»¶
            print(f"\n [{idx}/{len(tsv_paths)}] [è·³é] ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{os.path.basename(path)}")
            continue

        now_process = f"\n [{idx}/{len(tsv_paths)}] æ­£åœ¨è™•ç†ï¼š{tsv_name}"
        print(now_process)
        missing_data_logs.append(now_process)  # ğŸš€ ä¼˜åŒ–ï¼šæ”¶é›†æ—¥å¿—ï¼Œç¨åæ‰¹é‡å†™å…¥

        # å¦‚æœ append ä¸º Trueï¼Œåˆ™è¿›è¡Œç­›é€‰ (update mode processes all files)
        if append and update_ç°¡ç¨±_list and tsv_name not in update_ç°¡ç¨±_list:
            print(f"è·³éï¼š{tsv_name} (ä¸åœ¨å¾…æ›´æ–°æ¸…å–®ä¸­)")
            continue

        try:
            df = extract_all_from_files(path, query_db_path=query_db_path)
            print(f"  ğŸ“„ æå–è³‡æ–™è¡¨ï¼š{len(df)} è¡Œ")

            df = df.fillna("")
            df["æ¼¢å­—"] = df["æ±‰å­—"].astype(str).str.strip()
            df["éŸ³ç¯€"] = df["éŸ³æ ‡"].astype(str).str.strip()
            df["è²æ¯"] = df["å£°æ¯"].astype(str).str.strip()
            df["éŸ»æ¯"] = df["éŸµæ¯"].astype(str).str.strip()
            df["è²èª¿"] = df["å£°è°ƒ"].astype(str).str.strip()
            df["è¨»é‡‹"] = df["è¨»é‡‹"].astype(str).str.strip() if "è¨»é‡‹" in df.columns else ""

            # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–æ“ä½œè¿‡æ»¤æ•°æ®ï¼Œé¿å… iterrows()
            # 1. è¿‡æ»¤ï¼šè‡³å°‘æœ‰ä¸€ä¸ªéŸ³éŸµç‰¹å¾ä¸ä¸ºç©º
            has_any = (df["è²æ¯"] != "") | (df["éŸ»æ¯"] != "") | (df["è²èª¿"] != "")
            df_valid = df[has_any].copy()

            # 2. æ£€æµ‹ç¼ºå¤±æ•°æ®ï¼ˆæœ‰éƒ¨åˆ†éŸ³éŸµç‰¹å¾ä½†ä¸å®Œæ•´ï¼‰
            has_all = (df_valid["è²æ¯"] != "") & (df_valid["éŸ»æ¯"] != "") & (df_valid["è²èª¿"] != "")
            df_missing = df_valid[~has_all]

            # 3. æ‰¹é‡è®°å½•ç¼ºå¤±æ•°æ®æ—¥å¿—ï¼ˆé¿å…é¢‘ç¹æ–‡ä»¶I/Oï¼‰
            if len(df_missing) > 0:
                for row in df_missing.itertuples(index=False):
                    missing_data_logs.append(
                        f"â— ç¼ºè³‡æ–™ï¼šchar={row.æ¼¢å­—}, éŸ³ç¯€={row.éŸ³ç¯€}, è²æ¯='{row.è²æ¯}', éŸ»æ¯='{row.éŸ»æ¯}', è²èª¿='{row.è²èª¿}'"
                    )

            # 4. ğŸš€ ä½¿ç”¨ itertuples() æ›¿ä»£ iterrows()ï¼ˆå¿«10-100å€ï¼‰
            batch_data = [
                (tsv_name, row.æ¼¢å­—, row.éŸ³ç¯€, row.è²æ¯, row.éŸ»æ¯, row.è²èª¿, row.è¨»é‡‹, "")
                for row in df_valid.itertuples(index=False)
            ]
            insert_count = len(batch_data)

            # æ‰¹é‡æ’å…¥æ‰€æœ‰æ•°æ®
            if batch_data:
                cursor.executemany('''
                    INSERT INTO dialects (ç°¡ç¨±, æ¼¢å­—, éŸ³ç¯€, è²æ¯, éŸ»æ¯, è²èª¿, è¨»é‡‹, å¤šéŸ³å­—)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', batch_data)

            conn.commit()
            log_lines.append(f"{tsv_name} å¯«å…¥äº† {insert_count} ç­†ã€‚")
            # print(f" {tsv_name} å®Œæˆï¼šå…±å¯«å…¥ {insert_count} ç­†ã€‚")

            # Track processed ç°¡ç¨±
            if tsv_name not in processed_ç°¡ç¨±:
                processed_ç°¡ç¨±.append(tsv_name)

        except Exception as e:
            error_detail = traceback.format_exc()
            log_lines.append(f" {tsv_name} å¯«å…¥å¤±æ•—ï¼š\n{error_detail}")
            print(f" éŒ¯èª¤è™•ç† {tsv_name}ï¼š\n{error_detail}")

    conn.close()
    print(f"\nğŸ“¦ æ‰€æœ‰è³‡æ–™å·²å¯«å…¥ï¼š{db_path}")

    # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡å†™å…¥æ‰€æœ‰æ—¥å¿—ï¼ˆä¸€æ¬¡æ€§I/Oï¼‰
    if missing_data_logs:
        with open(MISSING_DATA_LOG, "a", encoding="utf-8") as f:
            f.write("\n".join(missing_data_logs) + "\n")

    #  ä¼˜åŒ–ï¼šé‡æ–°è¿æ¥å¹¶æ¢å¤æ­£å¸¸æ¨¡å¼ï¼Œç„¶ååˆ›å»ºç´¢å¼•
    conn_all = sqlite3.connect(db_path)
    cursor = conn_all.cursor()

    # æ¢å¤æ­£å¸¸åŒæ­¥æ¨¡å¼
    cursor.execute("PRAGMA synchronous = NORMAL")
    cursor.execute("PRAGMA journal_mode = DELETE")

    # å‰µå»ºç´¢å¼•ï¼ŒåŠ å¿«æŸ¥è©¢é€Ÿåº¦
    # update æ¨¡å¼ä¸‹è·³éå‰µå»ºç´¢å¼•ï¼ˆç´¢å¼•å·²å­˜åœ¨ï¼Œä¸éœ€è¦é‡æ–°å‰µå»ºï¼‰
    if not update:
        print("â€» é–‹å§‹å‰µå»ºç´¢å¼• â€»")
        # åŸºç¡€å•åˆ—ç´¢å¼•ï¼ˆFastAPI åç«¯é¢‘ç¹æŸ¥è¯¢çš„å­—æ®µï¼‰
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr ON dialects(ç°¡ç¨±);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_char ON dialects(æ¼¢å­—);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_syllable ON dialects(éŸ³ç¯€);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_polyphonic ON dialects(å¤šéŸ³å­—);")  # æ–°å¢ï¼šå¤šéŸ³å­—æŸ¥è¯¢

        # å¤åˆç´¢å¼•ï¼Œä¼˜åŒ–å¤šå­—æ®µæŸ¥è¯¢å’Œ GROUP BY
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_char_abbr ON dialects(æ¼¢å­—, ç°¡ç¨±);")  # FastAPI æœ€é‡è¦
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr_char ON dialects(ç°¡ç¨±, æ¼¢å­—);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr_char_syllable ON dialects(ç°¡ç¨±, æ¼¢å­—, éŸ³ç¯€);")

        # ã€ä¼˜å…ˆçº§é«˜ã€‘ç”¨äºéŸ³éŸµç‰¹å¾æŸ¥è¯¢ï¼ˆåˆ†åˆ«ä¼˜åŒ–è²æ¯/éŸ»æ¯/è²èª¿æŸ¥è¯¢ï¼‰
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr_initial ON dialects(ç°¡ç¨±, è²æ¯);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr_final ON dialects(ç°¡ç¨±, éŸ»æ¯);")
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_abbr_tone ON dialects(ç°¡ç¨±, è²èª¿);")

        # ã€ä¼˜å…ˆçº§é«˜ã€‘ä¼˜åŒ–å¤šéŸ³å­—æŸ¥è¯¢ï¼ˆWHERE å¤šéŸ³å­—='1' AND ç°¡ç¨±=? AND æ¼¢å­— IN ...ï¼‰
        conn_all.execute("CREATE INDEX IF NOT EXISTS idx_dialects_polyphonic_full ON dialects(å¤šéŸ³å­—, ç°¡ç¨±, æ¼¢å­—);")
        print("âœ… ç´¢å¼•å‰µå»ºå®Œæˆ")
    else:
        print("â­ï¸  update æ¨¡å¼ï¼šè·³éå‰µå»ºç´¢å¼•ï¼ˆç´¢å¼•å·²å­˜åœ¨ï¼‰")

    conn_all.commit()
    conn_all.close()  # ğŸ”§ ä¿®å¤ï¼šå…³é—­æ•°æ®åº“è¿æ¥ï¼Œé¿å…é”å®š

    # print("\n å¯«å…¥ç¸½çµï¼š")
    # for line in log_lines:
    # print("   " + line)

    with open(WRITE_INFO_LOG, "w", encoding="utf-8") as f:
        f.write("\n".join(log_lines))
    # print(f"\nğŸ“ å·²å¯«å…¥ç´€éŒ„è‡³ï¼š{log_path}")

    return processed_ç°¡ç¨±  # Return list of processed dialects



# ğŸš€ ä¼˜åŒ–ç‰ˆæœ¬ï¼šåˆ†æ‰¹è™•ç†ï¼Œé¿å…å…§å­˜æº¢å‡º
def process_polyphonic_annotations(db_path: str):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    #  ä¼˜åŒ–ï¼šè®¾ç½® SQLite æ€§èƒ½å‚æ•°
    cursor.execute("PRAGMA synchronous = OFF")
    cursor.execute("PRAGMA journal_mode = MEMORY")

    # å…ˆç²å–æ‰€æœ‰å”¯ä¸€çš„ç°¡ç¨±
    cursor.execute("SELECT DISTINCT ç°¡ç¨± FROM dialects")
    locations = [row[0] for row in cursor.fetchall()]
    total_locations = len(locations)

    print(f"ğŸ“ å…±æœ‰ {total_locations} å€‹åœ°é»å¾…è™•ç†\n")

    # å‰µå»ºè‡¨æ™‚è¡¨å­˜å„²è™•ç†å¾Œçš„çµæœ
    cursor.execute("DROP TABLE IF EXISTS dialects_temp")
    cursor.execute('''
        CREATE TABLE dialects_temp (
            ç°¡ç¨± TEXT,
            æ¼¢å­— TEXT,
            éŸ³ç¯€ TEXT,
            è²æ¯ TEXT,
            éŸ»æ¯ TEXT,
            è²èª¿ TEXT,
            è¨»é‡‹ TEXT,
            å¤šéŸ³å­— TEXT
        )
    ''')

    # åˆ†æ‰¹è™•ç†åœ°é»ï¼ˆæ¯æ¬¡è™•ç† 20 å€‹ï¼‰
    batch_size = 50
    total_batches = (total_locations + batch_size - 1) // batch_size

    for batch_idx in range(0, total_locations, batch_size):
        batch_locations = locations[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1

        print(f"\n[æ‰¹æ¬¡ {batch_num}/{total_batches}] æ­£åœ¨è™•ç† {len(batch_locations)} å€‹åœ°é»...")
        print(f"  åœ°é»ï¼š{', '.join(batch_locations[:5])}{'...' if len(batch_locations) > 5 else ''}")

        # è®€å–ç•¶å‰æ‰¹æ¬¡æ‰€æœ‰åœ°é»çš„æ•¸æ“š
        placeholders = ','.join(['?' for _ in batch_locations])
        df = pd.read_sql_query(
            f"SELECT * FROM dialects WHERE ç°¡ç¨± IN ({placeholders}) ORDER BY ç°¡ç¨±, æ¼¢å­—",
            conn,
            params=batch_locations
        )

        if len(df) == 0:
            continue

        print(f"  è®€å–äº† {len(df)} ç­†æ•¸æ“š")

        # ğŸš€ å‰µå»ºéŸ³éŸ»ç‰¹å¾µçµ„åˆéµ
        df['_phonetic_key'] = (df['è²æ¯'].astype(str) + '|' +
                               df['éŸ»æ¯'].astype(str) + '|' +
                               df['è²èª¿'].astype(str))

        # ğŸš€ ç‚ºæ¯å€‹ (ç°¡ç¨±, æ¼¢å­—, éŸ³ç¯€) çµ„åˆ†é…å”¯ä¸€ID
        df['_group_id'] = df.groupby(['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€']).ngroup()

        # ğŸš€ è¨ˆç®—æ¯çµ„çš„å”¯ä¸€éŸ³éŸ»ç‰¹å¾µæ•¸
        phonetic_counts = df.groupby('_group_id')['_phonetic_key'].nunique()
        df['_phonetic_count'] = df['_group_id'].map(phonetic_counts)

        # ğŸš€ åˆ†é›¢å…©ç¨®æƒ…æ³ï¼šéŸ³éŸ»ä¸€è‡´ vs éŸ³éŸ»ä¸ä¸€è‡´
        consistent_mask = df['_phonetic_count'] == 1
        inconsistent_mask = ~consistent_mask

        # === è™•ç†éŸ³éŸ»ä¸€è‡´çš„çµ„ï¼ˆéœ€è¦åˆä½µè¨»é‡‹ï¼‰ ===
        consistent_df = df[consistent_mask].copy()

        # ğŸš€ å‘é‡åŒ–åˆä½µè¨»é‡‹
        def agg_notes(series):
            """èšåˆè¨»é‡‹ï¼šå»ç©ºã€å»é‡ã€ç”¨åˆ†è™Ÿé€£æ¥"""
            notes = series.dropna().astype(str).str.strip()
            notes = notes[notes != '']
            if len(notes) == 0:
                return ''
            return ';'.join(notes.unique())

        # ğŸš€ ä½¿ç”¨ groupby.agg() ä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰çµ„
        if len(consistent_df) > 0:
            consistent_merged = consistent_df.groupby('_group_id', as_index=False).agg({
                'ç°¡ç¨±': 'first',
                'æ¼¢å­—': 'first',
                'éŸ³ç¯€': 'first',
                'è²æ¯': 'first',
                'éŸ»æ¯': 'first',
                'è²èª¿': 'first',
                'è¨»é‡‹': agg_notes,
                'å¤šéŸ³å­—': 'first'
            })
        else:
            consistent_merged = pd.DataFrame(columns=['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—'])

        # === è™•ç†éŸ³éŸ»ä¸ä¸€è‡´çš„çµ„ï¼ˆä¿ç•™æ‰€æœ‰è¡Œï¼‰ ===
        inconsistent_df = df[inconsistent_mask][['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—']].copy()

        # ğŸš€ åˆä½µå…©éƒ¨åˆ†
        merged_df = pd.concat([consistent_merged, inconsistent_df], ignore_index=True)

        # æ¨™è¨˜å¤šéŸ³å­—ï¼ˆéŸ³ç¯€ä¸åŒï¼‰- æŒ‰ç°¡ç¨±å’Œæ¼¢å­—åˆ†çµ„
        merged_df['å¤šéŸ³å­—'] = merged_df.groupby(['ç°¡ç¨±', 'æ¼¢å­—'])['éŸ³ç¯€'].transform(
            lambda x: '1' if x.nunique() > 1 else ''
        )

        # åªä¿ç•™éœ€è¦çš„åˆ—ï¼Œå»é™¤è‡¨æ™‚åˆ—
        final_columns = ['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—']
        merged_df = merged_df[final_columns]

        # å¯«å…¥è‡¨æ™‚è¡¨
        merged_df.to_sql("dialects_temp", conn, if_exists='append', index=False)

        print(f"  è™•ç†å¾Œå‰©é¤˜ {len(merged_df)} ç­†ï¼ˆåŸå§‹ {len(df)} ç­†ï¼‰")

        # é‡‹æ”¾å…§å­˜
        del df, merged_df

    print("\nâ³ æ­£åœ¨é‡å»ºæ•¸æ“šåº«è¡¨...")
    cursor.execute("DROP TABLE IF EXISTS dialects")
    cursor.execute("ALTER TABLE dialects_temp RENAME TO dialects")

    # æ¢å¤æ­£å¸¸æ¨¡å¼
    cursor.execute("PRAGMA synchronous = NORMAL")
    cursor.execute("PRAGMA journal_mode = DELETE")

    conn.commit()
    conn.close()
    print("âœ… å¤šéŸ³å­—è™•ç†å®Œæˆ")


# æ–°ä»£ç¢¼ï¼Œå¯¦æ™‚æ›´æ”¹æ•¸æ“šåº«ï¼ŒåŠ å¿«é‹è¡Œé€Ÿåº¦(å®é™…ä¸Šè¿˜å˜æ…¢äº†ã€‚ã€‚ï¼‰
def process_polyphonic_annotations_new(db_path: str, append: bool = False):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query("SELECT * FROM dialects ORDER BY ç°¡ç¨±, æ¼¢å­—", conn)
    # å¦‚æœ append æ¨¡å¼é–‹å•Ÿï¼Œåªä¿ç•™æŒ‡å®šç°¡ç¨±
    if append:
        try:
            df_append = pd.read_excel(APPEND_PATH, sheet_name="æª”æ¡ˆ")
            update_rows = df_append[df_append['å¾…æ›´æ–°'] == 1]
            valid_ç°¡ç¨± = update_rows['ç°¡ç¨±'].dropna().unique().tolist()

            if valid_ç°¡ç¨±:
                df = df[df["ç°¡ç¨±"].isin(valid_ç°¡ç¨±)]
                print(f"ğŸ“Œ åªè™•ç†å¾…æ›´æ–°ç°¡ç¨±ï¼š{valid_ç°¡ç¨±}")
            else:
                print("âš ï¸ APPEND_PATH ä¸­æœªç™¼ç¾ä»»ä½•å¾…æ›´æ–°ç°¡ç¨±ï¼Œè·³éè™•ç†ã€‚")
                conn.close()
                return
        except Exception as e:
            print(f"â— ç„¡æ³•è®€å– APPEND_PATHï¼š{e}ï¼Œå°‡è™•ç†å…¨éƒ¨è³‡æ–™ã€‚")

    print(f" å¾…è™•ç†è³‡æ–™ç­†æ•¸ï¼š{len(df)}")

    grouped = df.groupby(["ç°¡ç¨±", "æ¼¢å­—"])

    previous_short_name = None  # ç”¨æ¥ä¿å­˜ä¸Šä¸€æ¬¡çš„åœ°ç‚¹ä¿¡æ¯
    count_num = 1
    for (short_name, char), group in grouped:
        if short_name != previous_short_name:  # å½“åœ°ç‚¹å˜åŒ–æ—¶è§¦å‘
            print(f"æ­£åœ¨è™•ç†ï¼š{short_name}(ç¬¬{count_num}å€‹)")  # è¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œåœ°ç‚¹å‘ç”Ÿå˜åŒ–
            count_num += 1

        # ä¸€éšæ®µï¼šè™•ç†è¨»é‡‹
        grouped_syllables = group.groupby("éŸ³ç¯€")
        for syllable, syllable_group in grouped_syllables:
            unique_phonetics = syllable_group[["è²æ¯", "éŸ»æ¯", "è²èª¿"]].drop_duplicates()
            if len(unique_phonetics) == 1:
                notes = syllable_group["è¨»é‡‹"].dropna().astype(str).str.strip().unique()
                notes = [n for n in notes if n]
                combined_note = ";".join(notes) if notes else ""

                base_row = syllable_group.iloc[0].copy()
                base_row["è¨»é‡‹"] = combined_note

                # æ›´æ–°è³‡æ–™åº«ä¸­çš„è¨»é‡‹
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE dialects
                    SET è¨»é‡‹ = ?
                    WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                """, (combined_note, short_name, char, syllable))

                # åˆªé™¤é™¤ç¬¬ä¸€è¡Œå¤–çš„å…¶ä»–é‡è¤‡è¡Œ
                cursor.execute("""
                    DELETE FROM dialects
                    WHERE (ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?)
                      AND rowid NOT IN (
                        SELECT MIN(rowid)
                        FROM dialects
                        WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                      )
                """, (short_name, char, syllable, short_name, char, syllable))

            else:
                print(f"âš ï¸ éŸ³ç¯€ç›¸åŒä½†è²éŸ»èª¿ä¸åŒï¼š{char} / {syllable}")
                for _, row in syllable_group.iterrows():
                    cursor = conn.cursor()
                    cursor.execute("""
                        UPDATE dialects
                        SET è¨»é‡‹ = ?
                        WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ? AND éŸ³ç¯€ = ?
                    """, (row["è¨»é‡‹"], short_name, char, syllable))

        # äºŒéšæ®µï¼šæ¨™è¨˜å¤šéŸ³å­—ï¼ˆéŸ³ç¯€ä¸åŒï¼‰
        syllables_count = len(group["éŸ³ç¯€"].unique())
        if syllables_count > 1:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE dialects
                SET å¤šéŸ³å­— = '1'
                WHERE ç°¡ç¨± = ? AND æ¼¢å­— = ?
            """, (short_name, char))

        previous_short_name = short_name  # æ›´æ–°ä¹‹å‰çš„åœ°ç‚¹

    conn.commit()
    print("ğŸ”¨ è³‡æ–™åº«æ›´æ–°å®Œæˆï¼")

    conn.close()


def sync_dialects_flags(all_db_path=DIALECTS_DB_PATH,
                        query_db_path=QUERY_DB_PATH,
                        log_path=CHARACTERS_DB_PATH):
    # è®€å– dialects_all.db ä¸­æ‰€æœ‰å”¯ä¸€ç°¡ç¨±
    conn_all = sqlite3.connect(all_db_path)
    #  ä¼˜åŒ–ï¼šç´¢å¼•å·²åœ¨ process_all2sql ä¸­åˆ›å»ºï¼Œæ­¤å¤„æ— éœ€é‡å¤
    cursor_all = conn_all.cursor()
    cursor_all.execute("SELECT DISTINCT ç°¡ç¨± FROM dialects")
    all_tags = set(row[0] for row in cursor_all.fetchall())
    conn_all.close()

    # è®€å– dialects_query.db ä¸­æ‰€æœ‰ç°¡ç¨±
    conn_query = sqlite3.connect(query_db_path)
    cursor_query = conn_query.cursor()

    # ç¢ºä¿å­˜å„²æ¨™è¨˜æ¬„ä½å­˜åœ¨
    cursor_query.execute("PRAGMA table_info(dialects)")
    columns = [col[1] for col in cursor_query.fetchall()]
    if "å­˜å„²æ¨™è¨˜" not in columns:
        cursor_query.execute("ALTER TABLE dialects ADD COLUMN å­˜å„²æ¨™è¨˜ INTEGER DEFAULT 0")

    cursor_query.execute("SELECT rowid, ç°¡ç¨± FROM dialects")
    query_map = {tag: rowid for rowid, tag in cursor_query.fetchall()}

    matched = []
    unmatched = []

    for tag in sorted(all_tags):
        if tag in query_map:
            rowid = query_map[tag]
            cursor_query.execute("UPDATE dialects SET å­˜å„²æ¨™è¨˜ = 1 WHERE rowid = ?", (rowid,))
            matched.append(tag)
        else:
            unmatched.append(tag)
            print(f"â— ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{tag}")

    conn_query.commit()
    conn_query.close()

    # å¯«å…¥ log æª”æ¡ˆï¼ˆå‰é¢å…©å€‹ç©ºè¡Œï¼‰
    with open(log_path, "a", encoding="utf-8") as f:
        f.write("\n\n")
        for tag in unmatched:
            f.write(f"ç„¡æ³•åŒ¹é…ç°¡ç¨±ï¼š{tag}\n")

        # å¯«å…¥æˆåŠŸå­˜å„²è¨Šæ¯ï¼Œæ¯ 10 å€‹æ›è¡Œ
        lines = []
        for i in range(0, len(matched), 10):
            lines.append(", ".join(matched[i:i + 10]))
        success_message = "æˆåŠŸå­˜å„²ï¼š\n" + "\n".join(lines)
        f.write(success_message + "\n")

    print(" åŒæ­¥å®Œæˆã€‚å·²æ›´æ–°å­˜å„²æ¨™è¨˜ã€‚")


def process_phonology_excel(
        excel_file=PHO_TABLE_PATH,
        sheet_name="å±¤ç´š",
        db_file=CHARACTERS_DB_PATH,
        log_file=WRITE_INFO_LOG
):
    os.makedirs("data", exist_ok=True)

    # æ¬„ä½è¨­ç½®
    columns_needed = ["æ”", "å‘¼", "ç­‰", "éŸ»", "å…¥", "èª¿", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "éƒ¨ä½", "æ–¹å¼", "å–®å­—", "é‡‹ç¾©", "å¤šè²æ¯", "å¤šç­‰", "å¤šéŸ»", "å¤šèª¿"]
    rename_map = {"å–®å­—": "æ¼¢å­—"}
    write_columns = ["æ”", "å‘¼", "ç­‰", "éŸ»", "å…¥", "èª¿", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "éƒ¨ä½", "æ–¹å¼", "æ¼¢å­—", "é‡‹ç¾©", "å¤šè²æ¯", "å¤šç­‰", "å¤šéŸ»", "å¤šèª¿"]

    # è®€å– Excel
    try:
        df = pd.read_excel(excel_file, sheet_name=sheet_name, dtype=str)
    except Exception as e:
        print(f" è®€å– Excel å¤±æ•—: {e}")
        return

    try:
        df = df[columns_needed].rename(columns=rename_map)
    except KeyError as e:
        print(f" ç¼ºå°‘å¿…è¦æ¬„ä½: {e}")
        return

    # æ¸…é™¤æ¼¢å­—ç‚ºç©ºçš„è¡Œ
    df = df[df["æ¼¢å­—"].notna() & (df["æ¼¢å­—"].str.strip() != "")]
    df['num'] = df.index + 2  # Excel è¡Œè™Ÿ

    # æª¢æŸ¥å…¶ä»–æ¬„ä½æ˜¯å¦æœ‰ç¼ºå€¼ï¼ˆä¸åŒ…å«"æ¼¢å­—"èˆ‡"num"ï¼‰
    check_cols = [col for col in df.columns if col not in ["æ¼¢å­—", "num", "é‡‹ç¾©"]]
    invalid_rows = df[df[check_cols].isnull().any(axis=1)]

    # æœ‰æ•ˆåˆ—
    df_valid = df.drop(index=invalid_rows.index)

    # å»é™¤å®Œå…¨é‡è¤‡çš„åˆ—ï¼ˆåªæ¯”è¼ƒè¦å¯«å…¥çš„åˆ—ï¼‰
    df_unique = df_valid.drop_duplicates(subset=write_columns).copy()

    # æ¨™è¨˜ã€Œå¤šåœ°ä½ã€ï¼šåŒæ¼¢å­—å‡ºç¾å¤šæ¬¡ï¼ˆä½†è¡Œä¸åŒï¼‰
    dup_counts = df_unique["æ¼¢å­—"].value_counts()
    df_unique["å¤šåœ°ä½æ¨™è¨˜"] = df_unique["æ¼¢å­—"].map(lambda x: "1" if dup_counts.get(x, 0) > 1 else "")

    # è¼¸å‡ºéŒ¯èª¤è¨˜éŒ„
    if not invalid_rows.empty:
        invalid_output = invalid_rows[["num", "æ¼¢å­—"] + check_cols]
        print("â— ç™¼ç¾æ¬„ä½ç¼ºæ¼å¦‚ä¸‹ï¼š")
        print(invalid_output)

        with open(log_file, "a", encoding="utf-8") as f:
            f.write(invalid_output.to_csv(index=False, sep='\t', lineterminator='\n'))

    # å¯«å…¥ SQLite
    try:
        conn = sqlite3.connect(db_file)
        df_unique.drop(columns=["num"]).to_sql("characters", conn, if_exists="replace", index=False)
        print("â¤ é–‹å§‹å»ºç«‹ç´¢å¼•...")

        # 1. å®šä¹‰é‚£ 12 ä¸ªéœ€è¦ä¸"æ±‰å­—"ç»„åˆçš„æ ¸å¿ƒå±æ€§
        # è¿™äº›åˆ—å°†å»ºç«‹ (Col, æ¼¢å­—) çš„è”åˆç´¢å¼•
        composite_group = ["æ”", "å‘¼", "ç­‰", "éŸ»", "å…¥", "èª¿", "æ¸…æ¿", "ç³»", "çµ„", "æ¯", "éƒ¨ä½", "æ–¹å¼"]
        triple_indexes = [
            ("æ”", "ç­‰"),  # -> (æ”, ç­‰, æ¼¢å­—)
            ("æ”", "å‘¼"),  # -> (æ”, å‘¼, æ¼¢å­—)
            ("æ”", "æ¯"),  # -> (æ”, æ¯, æ¼¢å­—)
            ("æ¸…æ¿", "èª¿")  # -> (æ¸…æ¿, èª¿, æ¼¢å­—)
        ]
        for col1, col2 in triple_indexes:
            index_name = f"idx_characters_{col1}_{col2}_hanzi"
            # å‰µå»ºä¸‰åˆ—ç´¢å¼•
            sql = f"CREATE INDEX IF NOT EXISTS {index_name} ON characters({col1}, {col2}, æ¼¢å­—);"
            conn.execute(sql)
            print(f"   [ä¸‰åˆ—ç´¢å¼•] ({col1}, {col2}, æ¼¢å­—)")

        #  ã€ä¼˜å…ˆçº§ä¸­ã€‘ç”¨äº status_arrange_pho.py çš„åˆ†ç»„ç»Ÿè®¡ï¼ˆçµ„â†’æ¯â†’æ”â†’éŸ»â†’èª¿ï¼‰
        conn.execute("CREATE INDEX IF NOT EXISTS idx_characters_hierarchy ON characters(çµ„, æ¯, æ”, éŸ», èª¿);")
        print(f"   [äº”åˆ—ç´¢å¼•] (çµ„, æ¯, æ”, éŸ», èª¿) - ç”¨äºåˆ†ç»„ç»Ÿè®¡")

        #  ã€ä¼˜å…ˆçº§ä¸­ã€‘ç”¨äºç­‰çº§æŸ¥è¯¢ï¼ˆç­‰=ä¸‰çš„ç‰¹æ®Šå¤„ç†ï¼‰
        conn.execute("CREATE INDEX IF NOT EXISTS idx_characters_grade ON characters(ç­‰, æ¼¢å­—);")
        print(f"   [å¤åˆç´¢å¼•] (ç­‰, æ¼¢å­—) - ç”¨äºç­‰çº§æŸ¥è¯¢")

        # 2. å‡†å¤‡æ‰€æœ‰éœ€è¦å¤„ç†çš„åˆ—ï¼ˆæ’é™¤â€œé‡‹ç¾©â€ï¼ŒåŠ ä¸Šâ€œå¤šåœ°ä½æ¨™è¨˜â€ï¼‰
        all_index_candidates = [col for col in write_columns if col != "é‡‹ç¾©"]
        all_index_candidates.append("å¤šåœ°ä½æ¨™è¨˜")

        for col in all_index_candidates:
            if col == "æ¼¢å­—":
                # ã€ç‰¹æ®Šå¤„ç†ã€‘æ±‰å­—æœ¬èº«å¿…é¡»æœ‰å•åˆ—ç´¢å¼•
                # ç”¨äºï¼šWHERE æ¼¢å­— = 'æ±'
                index_name = "idx_characters_æ¼¢å­—"
                conn.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON characters(æ¼¢å­—);")
                print(f"   [å•åˆ—ç´¢å¼•] {col}")

            elif col in composite_group:
                # ã€æ ¸å¿ƒä¼˜åŒ–ã€‘è¿™ 12 åˆ—å»ºç«‹è”åˆç´¢å¼•ï¼š(å±æ€§, æ¼¢å­—)
                # ç”¨äºï¼šWHERE éŸ» = 'æ±' (åŒæ—¶è¦†ç›–äº† SELECT æ¼¢å­—)
                # æ³¨æ„ï¼šä¸å†å»ºç«‹ col çš„å•åˆ—ç´¢å¼•ï¼Œå› ä¸ºè”åˆç´¢å¼•çš„æœ€å·¦å‰ç¼€å·²ç»åŒ…å«äº†å•åˆ—æŸ¥è¯¢åŠŸèƒ½
                index_name = f"idx_characters_{col}_hanzi"
                conn.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON characters({col}, æ¼¢å­—);")
                print(f"   [è”åˆç´¢å¼•] ({col}, æ¼¢å­—)")

            else:
                # ã€å…¶ä»–åˆ—ã€‘ä¿ç•™å•åˆ—ç´¢å¼•
                # åŒ…æ‹¬ï¼šå¤šè²æ¯, å¤šç­‰, å¤šéŸ», å¤šèª¿, å¤šåœ°ä½æ¨™è¨˜
                index_name = f"idx_characters_{col}"
                conn.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON characters({col});")
                print(f"   [å•åˆ—ç´¢å¼•] {col}")

        conn.close()
        print(" ç´¢å¼•å„ªåŒ–å®Œæˆï¼")
    except Exception as e:
        print(f" SQLite å¯«å…¥å¤±æ•—: {e}")


def scan_update_directory():
    """
    Scan UPDATE_DATA_DIR for all TSV files
    Returns list of TSV file paths
    """
    if not os.path.exists(UPDATE_DATA_DIR):
        print(f"âš ï¸ UPDATE_DATA_DIR ä¸å­˜åœ¨: {UPDATE_DATA_DIR}")
        return []

    tsv_files = []
    for file in os.listdir(UPDATE_DATA_DIR):
        if file.endswith('.tsv'):
            full_path = os.path.join(UPDATE_DATA_DIR, file)
            tsv_files.append(full_path)

    print(f"ğŸ“‚ å¾ UPDATE_DATA_DIR æ‰¾åˆ° {len(tsv_files)} å€‹ TSV æ–‡ä»¶")
    return tsv_files


def process_polyphonic_annotations_selective(db_path: str, ç°¡ç¨±_list: list):
    """
    Process polyphonic annotations for specific dialects only
    More efficient than processing entire table

    Args:
        db_path: Path to dialects database
        ç°¡ç¨±_list: List of ç°¡ç¨± to process
    """
    if not ç°¡ç¨±_list:
        print("âš ï¸ æ²’æœ‰æŒ‡å®šè¦è™•ç†çš„ç°¡ç¨±ï¼Œè·³éå¤šéŸ³å­—è™•ç†")
        return

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Performance optimization
    cursor.execute("PRAGMA synchronous = OFF")
    cursor.execute("PRAGMA journal_mode = MEMORY")

    print(f"ğŸ“ è™•ç†å¤šéŸ³å­—æ¨™è¨˜ï¼ˆåƒ…è™•ç† {len(ç°¡ç¨±_list)} å€‹æ–¹è¨€é»ï¼‰...")

    # Process each ç°¡ç¨± separately to avoid loading entire table
    for idx, ç°¡ç¨± in enumerate(ç°¡ç¨±_list, 1):
        print(f"[{idx}/{len(ç°¡ç¨±_list)}] æ­£åœ¨è™•ç†ï¼š{ç°¡ç¨±}")

        # Read only this dialect's data
        query = "SELECT * FROM dialects WHERE ç°¡ç¨± = ? ORDER BY æ¼¢å­—"
        df = pd.read_sql_query(query, conn, params=(ç°¡ç¨±,))

        if df.empty:
            continue

        # Create phonetic key
        df['_phonetic_key'] = (df['è²æ¯'].astype(str) + '|' +
                               df['éŸ»æ¯'].astype(str) + '|' +
                               df['è²èª¿'].astype(str))

        # Group by (æ¼¢å­—, éŸ³ç¯€)
        df['_group_id'] = df.groupby(['æ¼¢å­—', 'éŸ³ç¯€']).ngroup()

        # Count unique phonetic features per group
        phonetic_counts = df.groupby('_group_id')['_phonetic_key'].nunique()
        df['_phonetic_count'] = df['_group_id'].map(phonetic_counts)

        # Process consistent groups (merge notes)
        consistent_mask = df['_phonetic_count'] == 1

        if consistent_mask.any():
            consistent_df = df[consistent_mask].copy()

            def agg_notes(series):
                notes = series.dropna().astype(str).str.strip()
                notes = notes[notes != '']
                if len(notes) == 0:
                    return ''
                return ';'.join(notes.unique())

            consistent_merged = consistent_df.groupby('_group_id', as_index=False).agg({
                'ç°¡ç¨±': 'first',
                'æ¼¢å­—': 'first',
                'éŸ³ç¯€': 'first',
                'è²æ¯': 'first',
                'éŸ»æ¯': 'first',
                'è²èª¿': 'first',
                'è¨»é‡‹': agg_notes,
                'å¤šéŸ³å­—': 'first'
            })

            # Delete old records for this ç°¡ç¨±
            cursor.execute("DELETE FROM dialects WHERE ç°¡ç¨± = ?", (ç°¡ç¨±,))

            # Insert merged records
            inconsistent_df = df[~consistent_mask][['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—']].copy()
            merged_df = pd.concat([consistent_merged, inconsistent_df], ignore_index=True)
        else:
            # No merging needed, just use original data
            cursor.execute("DELETE FROM dialects WHERE ç°¡ç¨± = ?", (ç°¡ç¨±,))
            merged_df = df[['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—']].copy()

        # Mark polyphonic characters
        merged_df['å¤šéŸ³å­—'] = merged_df.groupby('æ¼¢å­—')['éŸ³ç¯€'].transform(
            lambda x: '1' if x.nunique() > 1 else ''
        )

        # åªä¿ç•™éœ€è¦çš„åˆ—ï¼Œå»é™¤è‡¨æ™‚åˆ—
        final_columns = ['ç°¡ç¨±', 'æ¼¢å­—', 'éŸ³ç¯€', 'è²æ¯', 'éŸ»æ¯', 'è²èª¿', 'è¨»é‡‹', 'å¤šéŸ³å­—']
        merged_df = merged_df[final_columns]

        # Re-insert processed data
        merged_df.to_sql('dialects', conn, if_exists='append', index=False)

    conn.commit()

    # Restore normal mode
    cursor.execute("PRAGMA synchronous = NORMAL")
    cursor.execute("PRAGMA journal_mode = DELETE")

    conn.close()
    print("âœ… å¤šéŸ³å­—è™•ç†å®Œæˆ")


def write_to_sql(yindian=None, write_chars_db=None, append=False, update=False, mode='admin'):
    """
    Args:
        mode: 'admin' æˆ– 'user'
        append: å¾ Excel é…ç½®æ–‡ä»¶è®€å–å¾…æ›´æ–°åˆ—è¡¨
        update: å¾ UPDATE_DATA_DIR ç›®éŒ„è®€å–æ‰€æœ‰ TSV æ–‡ä»¶é€²è¡Œå¢é‡æ›´æ–°
    """

    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    step_times = {}

    # 1. ç¢ºå®šæ•¸æ“šåº«è·¯å¾‘
    if mode == 'admin':
        query_db_path = QUERY_DB_ADMIN_PATH
        dialects_db_path = DIALECTS_DB_ADMIN_PATH
    else:  # user
        query_db_path = QUERY_DB_USER_PATH
        dialects_db_path = DIALECTS_DB_USER_PATH

    # 2. æ§‹å»º query æ•¸æ“šåº«ï¼ŒåŒæ™‚ç²å– TSV è·¯å¾‘åˆ—è¡¨
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ1ï¼šæ§‹å»ºæ–¹è¨€æŸ¥è©¢æ•¸æ“šåº«ï¼ˆ{mode} æ¨¡å¼ï¼‰...")
    print(f"{'='*60}")
    step1_start = time.time()
    tsv_paths = build_dialect_database(mode=mode)
    step_times['æ­¥é©Ÿ1ï¼šæ§‹å»ºæ–¹è¨€æŸ¥è©¢æ•¸æ“šåº«'] = time.time() - step1_start

    # 3. Override TSV paths if in update mode
    if update:
        print(f"\n{'='*60}")
        print(f"update æ¨¡å¼ï¼šä½¿ç”¨ UPDATE_DATA_DIR ä¸­çš„æ–‡ä»¶")
        print(f"{'='*60}")
        tsv_paths = scan_update_directory()

    # 4. éæ¿¾æ’é™¤æ–‡ä»¶ (only for non-update mode)
    if not update:
        tsv_paths = [
            p for p in tsv_paths
            if os.path.splitext(os.path.basename(p))[0] not in exclude_files
        ]

    print(f"   å…± {len(tsv_paths)} å€‹ TSV æ–‡ä»¶å¾…è™•ç†")

    # 5. å¯«å…¥ç¸½æ•¸æ“šè¡¨
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ2ï¼šå¯«å…¥æ–¹è¨€æ•¸æ“š...")
    print(f"{'='*60}")
    step2_start = time.time()
    db_path = os.path.join(os.getcwd(), dialects_db_path)
    processed_ç°¡ç¨± = process_all2sql(tsv_paths, db_path, append, update, query_db_path=query_db_path)
    step_times['æ­¥é©Ÿ2ï¼šå¯«å…¥æ–¹è¨€æ•¸æ“š'] = time.time() - step2_start

    # 5. è™•ç†é‡è¤‡è¡Œå’Œå¤šéŸ³å­—
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ3ï¼šè™•ç†é‡è¤‡è¡Œå’Œå¤šéŸ³å­—...")
    print(f"{'='*60}")
    step3_start = time.time()

    if update and processed_ç°¡ç¨±:
        # Use selective processing for update mode (more efficient)
        process_polyphonic_annotations_selective(dialects_db_path, processed_ç°¡ç¨±)
    else:
        # Use full processing for normal/append mode
        process_polyphonic_annotations(dialects_db_path)

    step_times['æ­¥é©Ÿ3ï¼šè™•ç†é‡è¤‡è¡Œå’Œå¤šéŸ³å­—'] = time.time() - step3_start

    # 6. åŒæ­¥å­˜å„²æ¨™è¨˜
    print(f"\n{'='*60}")
    print(f"æ­¥é©Ÿ4ï¼šåŒæ­¥å­˜å„²æ¨™è¨˜...")
    print(f"{'='*60}")
    step4_start = time.time()
    sync_dialects_flags(
        all_db_path=dialects_db_path,
        query_db_path=query_db_path
    )
    step_times['æ­¥é©Ÿ4ï¼šåŒæ­¥å­˜å„²æ¨™è¨˜'] = time.time() - step4_start

    # 7. å¯«å…¥æ¼¢å­—åœ°ä½è¡¨ï¼ˆå¯é¸ï¼‰
    if write_chars_db:
        print(f"\n{'='*60}")
        print(f"æ­¥é©Ÿ5ï¼šå¯«å…¥æ¼¢å­—åœ°ä½è¡¨...")
        print(f"{'='*60}")
        step5_start = time.time()
        process_phonology_excel()
        step_times['æ­¥é©Ÿ5ï¼šå¯«å…¥æ¼¢å­—åœ°ä½è¡¨'] = time.time() - step5_start

    # è¨ˆç®—ç¸½æ™‚é–“
    total_time = time.time() - start_time

    # è¼¸å‡ºæ™‚é–“çµ±è¨ˆ
    print(f"\n{'='*60}")
    print(f"â±ï¸  åŸ·è¡Œæ™‚é–“çµ±è¨ˆ")
    print(f"{'='*60}")
    for step_name, duration in step_times.items():
        minutes = int(duration // 60)
        seconds = duration % 60
        if minutes > 0:
            print(f"  {step_name}: {minutes}åˆ†{seconds:.2f}ç§’")
        else:
            print(f"  {step_name}: {seconds:.2f}ç§’")

    print(f"{'-'*60}")
    total_minutes = int(total_time // 60)
    total_seconds = total_time % 60
    if total_minutes > 0:
        print(f"  âœ… ç¸½åŸ·è¡Œæ™‚é–“: {total_minutes}åˆ†{total_seconds:.2f}ç§’")
    else:
        print(f"  âœ… ç¸½åŸ·è¡Œæ™‚é–“: {total_seconds:.2f}ç§’")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    write_to_sql()
    # build_dialect_database()

